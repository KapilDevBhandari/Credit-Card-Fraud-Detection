# -*- coding: utf-8 -*-
"""credit card fraud detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QE3c5ADDqN7lUnMdgkOwwhc6kVcjPhb-

Importing The Dependencies
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# loading the csv data to a Pandas DataFrame
credit_card_data = pd.read_csv('/content/creditcard.csv')

# first 10 rows of the dataset
credit_card_data.head(10)

# last 10 rows of data
credit_card_data.tail(10)

# dataset informations
credit_card_data.info()

# checking the number of missing values in each column
credit_card_data.isnull().sum()

# distribution of legit transactions and fraudulent transactions
# where 0 is for legit transactions and 1 for fraudulent transactions
credit_card_data['Class'].value_counts()

"""This data set is not balanced data set as we can see the class o and 1 have huge difference around 99% data is  more in 0 then in 1 as 0 indicate legit and 1 indicate fraud so if we train this to our model it will not predict the fraudulent transactions.

0 --> Normal Transaction

1 --> Fraudulent Transaction
"""

# separating the data for analysis
# in this the if the value of class is 0 then all the data of class 0 will store in legit
# and if the class value 1 will store in fraud
legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]

print(legit.shape)
print(fraud.shape)

# statistical measures of the data
# this will help to work with amount in  legit transaction
legit.Amount.describe()

# this will help to work with amount in fraud transaction
fraud.Amount.describe()

# compare the values of both transaction
credit_card_data.groupby('Class').mean()

"""The difference between the mean of legit and fraud transaction is very huge.

# Under-Sampling

Build a sample dataset containing similar distribution of normal transactions and fraudulent transactions.

The number of fraudulent transactions --> 492
"""

legit_sample = legit.sample(n=492)

"""Concatenating two DataFrames"""

new_dataset = pd.concat([legit_sample, fraud], axis=0)

new_dataset.head()

new_dataset.tail()

new_dataset['Class'].value_counts()

"""Now the data is balanced. Which is distributed equaly so its will be easy to train and test."""

new_dataset.groupby('Class').mean()

"""Splitting the data into Features and Targets."""

X = new_dataset.drop(columns='Class', axis=1)
Y = new_dataset['Class']

print(X)

print(Y)

"""Split the data into Training data and Testing Data

"""

# we are creating x feature and y label and making them to x train and y train stored 80% of data of  feature and label
# and the 20 % of the data is stored in test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)
# 80% out of 984 goes to train and 20% goes to test

"""# Model Training

Logistic Regression
"""

from sklearn.preprocessing import StandardScaler

# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create model with adjusted parameters
model = LogisticRegression(max_iter=1000, solver='lbfgs', tol=1e-4)

model.fit(X_train_scaled, Y_train)

"""# Model Evaluation

Accuracy Score
"""

# accuracy on training data
X_train_prediction = model.predict(X_train_scaled)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data : ', training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test_scaled)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy on Test data : ', test_data_accuracy)

